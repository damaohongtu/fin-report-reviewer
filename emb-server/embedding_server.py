"""
HTTP EmbeddingæœåŠ¡

æä¾›HTTPæ¥å£çš„embeddingæœåŠ¡ï¼Œä¾›è´¢æŠ¥ç‚¹è¯„ç³»ç»Ÿè°ƒç”¨
ä½¿ç”¨FastAPI + SentenceTransformerå®ç°

å¯åŠ¨æ–¹å¼ï¼š
python emb-server/embedding_server.py --host 0.0.0.0 --port 8080 --device cuda --cache-dir ./models

æˆ–ä½¿ç”¨uvicornï¼š
uvicorn embedding_server:app --host 0.0.0.0 --port 8080 --reload

å‚æ•°è¯´æ˜ï¼š
--host: æœåŠ¡host (é»˜è®¤: 0.0.0.0)
--port: æœåŠ¡ç«¯å£ (é»˜è®¤: 8080)
--model: æ¨¡å‹åç§°æˆ–è·¯å¾„ (é»˜è®¤: BAAI/bge-base-zh-v1.5)
--device: è¿è¡Œè®¾å¤‡ cuda/cpu (é»˜è®¤: cuda if available)
--cache-dir: æ¨¡å‹ç¼“å­˜ç›®å½• (å¯é€‰ï¼ŒæŒ‡å®šåä¼šå°†æ¨¡å‹ä¸‹è½½åˆ°è¯¥ç›®å½•)
--workers: å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 1)
--reload: å¼€å¯çƒ­é‡è½½ï¼Œç”¨äºå¼€å‘æ¨¡å¼
"""

import argparse
import os
from typing import List, Optional
from pathlib import Path
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from sentence_transformers import SentenceTransformer
import torch
from loguru import logger

# ==================== é…ç½® ====================
DEFAULT_MODEL = "BAAI/bge-base-zh-v1.5"
DEFAULT_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DEFAULT_BATCH_SIZE = 32
DEFAULT_CACHE_DIR = None  # é»˜è®¤ä½¿ç”¨ç³»ç»Ÿç¼“å­˜ç›®å½•

# ==================== APIæ¨¡å‹ ====================

class EmbeddingRequest(BaseModel):
    """Embeddingè¯·æ±‚"""
    texts: List[str] = Field(..., description="æ–‡æœ¬åˆ—è¡¨", min_items=1)
    model: Optional[str] = Field(default=DEFAULT_MODEL, description="æ¨¡å‹åç§°")
    batch_size: Optional[int] = Field(default=DEFAULT_BATCH_SIZE, description="æ‰¹å¤„ç†å¤§å°")

class EmbeddingResponse(BaseModel):
    """Embeddingå“åº”"""
    embeddings: List[List[float]] = Field(..., description="å‘é‡åˆ—è¡¨")
    model: str = Field(..., description="æ¨¡å‹åç§°")
    dimension: int = Field(..., description="å‘é‡ç»´åº¦")
    count: int = Field(..., description="å‘é‡æ•°é‡")

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    model: str = Field(..., description="å½“å‰åŠ è½½çš„æ¨¡å‹")
    dimension: int = Field(..., description="å‘é‡ç»´åº¦")
    device: str = Field(..., description="è¿è¡Œè®¾å¤‡")
    cache_dir: Optional[str] = Field(None, description="ç¼“å­˜ç›®å½•")

# ==================== FastAPIåº”ç”¨ ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # Startup: åˆå§‹åŒ–æ¨¡å‹
    logger.info("ğŸš€ EmbeddingæœåŠ¡å¯åŠ¨ä¸­...")
    if embedding_model is None:
        logger.info("æ£€æµ‹åˆ°é€šè¿‡uvicornç›´æ¥å¯åŠ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–æ¨¡å‹")
        # ä½¿ç”¨é»˜è®¤æ¨¡å‹å’Œè®¾å¤‡ï¼Œä¸æŒ‡å®šcache_dirï¼ˆä½¿ç”¨ç³»ç»Ÿé»˜è®¤ï¼‰
        init_model(DEFAULT_MODEL, DEFAULT_DEVICE, cache_folder=None)
    yield
    # Shutdown: æ¸…ç†èµ„æºï¼ˆå¦‚æœéœ€è¦ï¼‰
    logger.info("ğŸ›‘ EmbeddingæœåŠ¡å…³é—­ä¸­...")

app = FastAPI(
    title="Embedding Service",
    description="æä¾›æ–‡æœ¬å‘é‡åŒ–æœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# å…¨å±€å˜é‡
embedding_model: Optional[SentenceTransformer] = None
model_name: str = DEFAULT_MODEL
device: str = DEFAULT_DEVICE
dimension: int = 768  # bge-base-zh-v1.5 çš„ç»´åº¦
cache_dir: Optional[str] = None


def init_model(model_path: str, device_name: str, cache_folder: Optional[str] = None):
    """åˆå§‹åŒ–embeddingæ¨¡å‹"""
    global embedding_model, model_name, device, dimension, cache_dir
    
    try:
        logger.info(f"æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {model_path}")
        logger.info(f"è®¾å¤‡: {device_name}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
        is_local_path = Path(model_path).exists()
        
        if is_local_path:
            # æœ¬åœ°è·¯å¾„ï¼šç›´æ¥ä»æŒ‡å®šè·¯å¾„åŠ è½½ï¼Œä¸ä½¿ç”¨cache_folder
            logger.info(f"ğŸ“‚ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„: {model_path}")
            model_abs_path = str(Path(model_path).resolve())
            logger.info(f"ğŸ“‚ ç»å¯¹è·¯å¾„: {model_abs_path}")
            cache_dir = str(Path(model_path).parent.resolve())  # è®°å½•æ¨¡å‹æ‰€åœ¨ç›®å½•
            
            embedding_model = SentenceTransformer(
                model_abs_path, 
                device=device_name
            )
            model_name = model_abs_path
        else:
            # HuggingFaceæ¨¡å‹åï¼šä½¿ç”¨cache_folder
            logger.info(f"ğŸŒ ä»HuggingFaceåŠ è½½: {model_path}")
            
            # è®¾ç½®ç¼“å­˜ç›®å½•
            if cache_folder:
                cache_path = Path(cache_folder)
                cache_path.mkdir(parents=True, exist_ok=True)
                os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(cache_path)
                os.environ['TRANSFORMERS_CACHE'] = str(cache_path)
                cache_dir = str(cache_path)
                logger.info(f"ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•: {cache_dir}")
            
            # è®¾ç½®HuggingFaceé•œåƒ
            if 'HF_ENDPOINT' not in os.environ:
                os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
                logger.info("ğŸŒ å·²è®¾ç½®HuggingFaceé•œåƒ: https://hf-mirror.com")
            
            embedding_model = SentenceTransformer(
                model_path, 
                device=device_name,
                cache_folder=cache_folder
            )
            model_name = model_path
        
        device = device_name
        
        # è·å–å‘é‡ç»´åº¦
        test_embedding = embedding_model.encode(["æµ‹è¯•"], convert_to_numpy=True)
        dimension = test_embedding.shape[1]
        
        logger.success(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        logger.success(f"âœ… å‘é‡ç»´åº¦: {dimension}")
        logger.success(f"âœ… è¿è¡Œè®¾å¤‡: {device}")
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    if embedding_model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    return HealthResponse(
        status="healthy",
        model=model_name,
        dimension=dimension,
        device=device,
        cache_dir=cache_dir
    )


@app.post("/embeddings", response_model=EmbeddingResponse)
async def generate_embeddings(request: EmbeddingRequest):
    """ç”Ÿæˆæ–‡æœ¬embeddings"""
    if embedding_model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    
    try:
        logger.info(f"æ”¶åˆ°embeddingè¯·æ±‚: {len(request.texts)}ä¸ªæ–‡æœ¬")
        
        # éªŒè¯æ–‡æœ¬
        if not request.texts:
            raise HTTPException(status_code=400, detail="æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # ç”Ÿæˆembeddings
        embeddings = embedding_model.encode(
            request.texts,
            batch_size=request.batch_size or DEFAULT_BATCH_SIZE,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        embeddings_list = embeddings.tolist()
        
        logger.success(f"âœ… ç”Ÿæˆäº†{len(embeddings_list)}ä¸ªembeddings")
        
        return EmbeddingResponse(
            embeddings=embeddings_list,
            model=model_name,
            dimension=dimension,
            count=len(embeddings_list)
        )
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆembeddingså¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆembeddingså¤±è´¥: {str(e)}")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "service": "Embedding Service",
        "version": "1.0.0",
        "model": model_name,
        "dimension": dimension,
        "device": device,
        "cache_dir": cache_dir,
        "endpoints": {
            "health": "/health",
            "embeddings": "/embeddings",
            "docs": "/docs"
        }
    }


# ==================== å‘½ä»¤è¡Œå¯åŠ¨ ====================

def main():
    """å‘½ä»¤è¡Œå¯åŠ¨"""
    parser = argparse.ArgumentParser(description="Embedding HTTPæœåŠ¡")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡host")
    parser.add_argument("--port", type=int, default=8080, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--device", type=str, default=DEFAULT_DEVICE, help="è¿è¡Œè®¾å¤‡ (cuda/cpu)")
    parser.add_argument("--cache-dir", type=str, default=None, help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    parser.add_argument("--workers", type=int, default=1, help="å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--reload", action="store_true", help="å¼€å¯çƒ­é‡è½½ï¼ˆå¼€å‘æ¨¡å¼ï¼‰")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨¡å‹
    init_model(args.model, args.device, args.cache_dir)
    
    # å¯åŠ¨æœåŠ¡
    import uvicorn
    
    logger.info(f"ğŸš€ å¯åŠ¨EmbeddingæœåŠ¡: {args.host}:{args.port}")
    
    uvicorn.run(
        "embedding_server:app",
        host=args.host,
        port=args.port,
        workers=args.workers,
        reload=args.reload,
        log_level="info"
    )


if __name__ == "__main__":
    main()

