"""
Aè‚¡è´¢æŠ¥æ•°æ®å¯¼å…¥å·¥å…·
ä»Excelæ–‡ä»¶å¯¼å…¥è´¢æŠ¥æ•°æ®åˆ°PostgreSQLæ•°æ®åº“ï¼ˆashare schemaï¼‰
"""

import pandas as pd
import psycopg2
from psycopg2.extras import execute_batch
from typing import Dict, List, Optional
from pathlib import Path
import logging
from dotenv import load_dotenv
import os
import sys
from urllib.parse import urlparse
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FinancialDataImporter:
    """Aè‚¡è´¢æŠ¥æ•°æ®å¯¼å…¥å™¨"""
    
    # åŸºç¡€å­—æ®µæ˜ å°„ï¼šExcelå­—æ®µå -> æ•°æ®åº“å­—æ®µå
    BASE_FIELD_MAPPING = {
        'Stkcd': 'stkcd',
        'ShortName': 'short_name',
        'Accper': 'accper',
        'Typrep': 'typrep',
        'IfCorrect': 'if_correct',
        'DeclareDate': 'declare_date',
    }
    
    def __init__(self, database_url: Optional[str] = None):
        """
        åˆå§‹åŒ–å¯¼å…¥å™¨
        
        Args:
            database_url: æ•°æ®åº“URLï¼Œæ ¼å¼ï¼špostgresql://user:password@host:port/database
                        å¦‚æœä¸æä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        self.database_url = database_url or os.getenv('DATABASE_URL')
        if not self.database_url:
            raise ValueError("æœªæ‰¾åˆ°æ•°æ®åº“é…ç½®ï¼Œè¯·è®¾ç½® DATABASE_URL ç¯å¢ƒå˜é‡")
        
        self.conn = None
        self._parse_db_config()
    
    def _parse_db_config(self):
        """è§£ææ•°æ®åº“é…ç½®"""
        parsed = urlparse(self.database_url)
        self.db_config = {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'database': parsed.path.lstrip('/'),
            'user': parsed.username,
            'password': parsed.password
        }
        logger.info(f"æ•°æ®åº“é…ç½®: {self.db_config['user']}@{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}")
    
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = psycopg2.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database'],
                user=self.db_config['user'],
                password=self.db_config['password']
            )
            logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.connect()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.close()
    
    def map_column_names(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ˜ å°„Excelåˆ—ååˆ°æ•°æ®åº“å­—æ®µå
        
        Args:
            df: åŸå§‹DataFrame
            
        Returns:
            åˆ—åå·²æ˜ å°„çš„DataFrame
        """
        column_mapping = self.BASE_FIELD_MAPPING.copy()
        
        # å¯¹äºè´¢åŠ¡æŒ‡æ ‡å­—æ®µï¼ˆå¦‚A001101000, B001100000ç­‰ï¼‰ï¼Œç»Ÿä¸€è½¬ä¸ºå°å†™
        for col in df.columns:
            if col not in column_mapping:
                # å¦‚æœä¸åœ¨é¢„å®šä¹‰æ˜ å°„ä¸­ï¼Œè½¬ä¸ºå°å†™
                column_mapping[col] = col.lower()
        
        # é‡å‘½ååˆ—
        df_mapped = df.rename(columns=column_mapping)
        
        logger.info(f"ğŸ“‹ åˆ—åæ˜ å°„å®Œæˆï¼Œå…± {len(df.columns)} åˆ—")
        logger.debug(f"æ˜ å°„ç¤ºä¾‹: {list(column_mapping.items())[:5]}")
        
        return df_mapped
    
    def prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ•°æ®é¢„å¤„ç†
        
        Args:
            df: åŸå§‹DataFrame
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        df_clean = df.copy()
        
        # å¤„ç†è‚¡ç¥¨ä»£ç ï¼ˆè¡¥é½ä¸º6ä½ï¼‰
        if 'stkcd' in df_clean.columns:
            df_clean['stkcd'] = df_clean['stkcd'].astype(str).str.zfill(6)
            logger.info(f"ğŸ“‹ è‚¡ç¥¨ä»£ç æ ¼å¼åŒ–å®Œæˆï¼ˆè¡¥é½ä¸º6ä½ï¼‰")
        
        # å¤„ç†æ—¥æœŸå­—æ®µ
        if 'accper' in df_clean.columns:
            df_clean['accper'] = pd.to_datetime(df_clean['accper'], errors='coerce')
            logger.info(f"ğŸ“… æ—¥æœŸå­—æ®µå¤„ç†å®Œæˆ")
        
        # å¤„ç†æ•°å€¼å­—æ®µï¼ˆå°†NaNè½¬ä¸ºNoneï¼Œä»¥ä¾¿æ’å…¥NULLåˆ°æ•°æ®åº“ï¼‰
        numeric_cols = df_clean.select_dtypes(include=['float64', 'int64']).columns
        for col in numeric_cols:
            df_clean[col] = df_clean[col].where(pd.notna(df_clean[col]), None)
        
        # å¤„ç†å­—ç¬¦ä¸²å­—æ®µï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰
        string_cols = df_clean.select_dtypes(include=['object']).columns
        for col in string_cols:
            if col not in ['accper']:  # æ’é™¤æ—¥æœŸå­—æ®µ
                df_clean[col] = df_clean[col].apply(
                    lambda x: x.strip() if isinstance(x, str) else x
                )
        
        logger.info(f"ğŸ”§ æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå…± {len(df_clean):,} è¡Œ")
        return df_clean
    
    def import_balance_sheet(self, excel_file: str, sheet_name: str = 'Sheet1'):
        """
        å¯¼å…¥èµ„äº§è´Ÿå€ºè¡¨
        
        Args:
            excel_file: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: sheetåç§°
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š å¼€å§‹å¯¼å…¥èµ„äº§è´Ÿå€ºè¡¨: {excel_file}")
        logger.info(f"{'='*60}")
        
        # è¯»å–Excel
        df = pd.read_excel(excel_file, sheet_name=sheet_name)
        logger.info(f"ğŸ“– è¯»å–Excelå®Œæˆï¼Œå…± {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        # æ˜ å°„åˆ—å
        df_mapped = self.map_column_names(df)
        
        # æ•°æ®é¢„å¤„ç†
        df_clean = self.prepare_data(df_mapped)
        
        # æ’å…¥æ•°æ®åº“
        self._batch_insert(df_clean, 'ashare.a_share_balance_sheet', 
                          primary_keys=['stkcd', 'accper', 'typrep'])
        
        logger.info(f"âœ… èµ„äº§è´Ÿå€ºè¡¨å¯¼å…¥å®Œæˆ\n")
    
    def import_income_statement(self, excel_file: str, sheet_name: str = 'Sheet1'):
        """
        å¯¼å…¥åˆ©æ¶¦è¡¨
        
        Args:
            excel_file: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: sheetåç§°
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š å¼€å§‹å¯¼å…¥åˆ©æ¶¦è¡¨: {excel_file}")
        logger.info(f"{'='*60}")
        
        # è¯»å–Excel
        df = pd.read_excel(excel_file, sheet_name=sheet_name)
        logger.info(f"ğŸ“– è¯»å–Excelå®Œæˆï¼Œå…± {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        # æ˜ å°„åˆ—å
        df_mapped = self.map_column_names(df)
        
        # æ•°æ®é¢„å¤„ç†
        df_clean = self.prepare_data(df_mapped)
        
        # æ’å…¥æ•°æ®åº“
        self._batch_insert(df_clean, 'ashare.a_share_income_statement',
                          primary_keys=['stkcd', 'accper', 'typrep'])
        
        logger.info(f"âœ… åˆ©æ¶¦è¡¨å¯¼å…¥å®Œæˆ\n")
    
    def import_cashflow_statement(self, excel_file: str, sheet_name: str = 'Sheet1'):
        """
        å¯¼å…¥ç°é‡‘æµé‡è¡¨
        
        Args:
            excel_file: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: sheetåç§°
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š å¼€å§‹å¯¼å…¥ç°é‡‘æµé‡è¡¨: {excel_file}")
        logger.info(f"{'='*60}")
        
        # è¯»å–Excel
        df = pd.read_excel(excel_file, sheet_name=sheet_name)
        logger.info(f"ğŸ“– è¯»å–Excelå®Œæˆï¼Œå…± {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        
        # æ˜ å°„åˆ—å
        df_mapped = self.map_column_names(df)
        
        # æ•°æ®é¢„å¤„ç†
        df_clean = self.prepare_data(df_mapped)
        
        # æ’å…¥æ•°æ®åº“
        self._batch_insert(df_clean, 'ashare.a_share_cashflow_statement',
                          primary_keys=['stkcd', 'accper', 'typrep'])
        
        logger.info(f"âœ… ç°é‡‘æµé‡è¡¨å¯¼å…¥å®Œæˆ\n")
    
    def _batch_insert(self, df: pd.DataFrame, table_name: str, 
                     primary_keys: List[str], batch_size: int = 500):
        """
        æ‰¹é‡æ’å…¥æ•°æ®ï¼ˆæ”¯æŒå†²çªæ›´æ–°ï¼Œä¼˜åŒ–å¤§æ•°æ®é›†å¤„ç†ï¼‰
        
        Args:
            df: è¦æ’å…¥çš„DataFrame
            table_name: è¡¨åï¼ˆåŒ…å«schemaï¼‰
            primary_keys: ä¸»é”®å­—æ®µåˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤500ï¼Œé€‚åˆå¤§æ•°æ®é›†ï¼‰
        """
        if df.empty:
            logger.warning("âš ï¸  DataFrameä¸ºç©ºï¼Œè·³è¿‡æ’å…¥")
            return
        
        # è·å–åˆ—å
        columns = df.columns.tolist()
        
        # ç”ŸæˆSQL
        placeholders = ', '.join(['%s'] * len(columns))
        columns_str = ', '.join(columns)
        
        # ä½¿ç”¨ON CONFLICTå¤„ç†ä¸»é”®å†²çªï¼ˆæ›´æ–°æ•°æ®ï¼‰
        update_columns = [col for col in columns if col not in primary_keys and col not in ['created_at']]
        update_str = ', '.join([f"{col} = EXCLUDED.{col}" for col in update_columns])
        
        conflict_keys = ', '.join(primary_keys)
        
        sql = f"""
            INSERT INTO {table_name} ({columns_str})
            VALUES ({placeholders})
            ON CONFLICT ({conflict_keys}) 
            DO UPDATE SET {update_str}, updated_at = CURRENT_TIMESTAMP
        """
        
        # å‡†å¤‡æ•°æ®
        total_rows = len(df)
        logger.info(f"ğŸ’¾ å‡†å¤‡æ’å…¥ {total_rows:,} æ¡è®°å½•...")
        
        # åˆ†æ‰¹å¤„ç†
        cursor = self.conn.cursor()
        success_count = 0
        error_count = 0
        
        try:
            # åˆ†æ‰¹å¤„ç†æ•°æ®ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
            for i in tqdm(range(0, total_rows, batch_size), 
                         desc="æ’å…¥è¿›åº¦", 
                         unit="batch",
                         ncols=100):
                
                # è·å–å½“å‰æ‰¹æ¬¡çš„æ•°æ®
                batch_df = df.iloc[i:i+batch_size]
                batch_data = [tuple(x) for x in batch_df.to_numpy()]
                
                try:
                    # æ’å…¥å½“å‰æ‰¹æ¬¡
                    execute_batch(cursor, sql, batch_data, page_size=batch_size)
                    self.conn.commit()  # æ¯æ‰¹æ¬¡commitä¸€æ¬¡
                    success_count += len(batch_data)
                    
                except Exception as e:
                    self.conn.rollback()
                    error_count += len(batch_data)
                    logger.error(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±è´¥: {str(e)[:100]}")
                    continue
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥/æ›´æ–° {success_count:,} æ¡è®°å½•åˆ° {table_name}")
            if error_count > 0:
                logger.warning(f"âš ï¸  {error_count:,} æ¡è®°å½•æ’å…¥å¤±è´¥")
                
        except Exception as e:
            self.conn.rollback()
            logger.error(f"âŒ æ’å…¥æ•°æ®å¤±è´¥: {e}")
            raise
        finally:
            cursor.close()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Aè‚¡è´¢æŠ¥æ•°æ®å¯¼å…¥å·¥å…·')
    parser.add_argument('--table', '-t', required=True, 
                       choices=['balance_sheet', 'income_statement', 'cashflow_statement', 'all'],
                       help='è¦å¯¼å…¥çš„è¡¨ç±»å‹')
    parser.add_argument('--file', '-f', required=True,
                       help='Excelæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sheet', '-s', default='Sheet1',
                       help='Excelå·¥ä½œè¡¨åç§°ï¼ˆé»˜è®¤ï¼šSheet1ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.file).exists():
        logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        sys.exit(1)
    
    # æ‰§è¡Œå¯¼å…¥
    try:
        with FinancialDataImporter() as importer:
            if args.table == 'balance_sheet' or args.table == 'all':
                importer.import_balance_sheet(args.file, args.sheet)
            
            if args.table == 'income_statement' or args.table == 'all':
                importer.import_income_statement(args.file, args.sheet)
            
            if args.table == 'cashflow_statement' or args.table == 'all':
                importer.import_cashflow_statement(args.file, args.sheet)
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®å¯¼å…¥å®Œæˆï¼")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"\nâŒ å¯¼å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()

